{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyDAgg-FC_Wh"
      },
      "source": [
        "#Programming assignment #5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s1BB9VoDC8Y"
      },
      "source": [
        "## 1 Supervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbfgCOGMDKjo"
      },
      "source": [
        "> Here, you are going to use the features you generated in Assignment #3 to predict the\n",
        "clients response to a promotion campaign. This is a typical classification problem in the retail industry, but the formulation of the problem is similar to industries such as fraud detection, marketing and manufacturing.\n",
        ">\n",
        "> The clients responses are stored in the Retail_Data_Response.csv file from Kaggle. The responses are **binary: 0** for clients who responded negatively to the promotional campaign and **1** for clients who responded positively to the campaign.\n",
        ">\n",
        "> You will explore solving the classiâ€€cation problem with two different sets of features (i.e. annual and monthly) and three different algorithms as shown in the image below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsvwUogjDz9Z"
      },
      "source": [
        "### 1.1 Import the monthly and annual data and join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWijzfQID66p"
      },
      "source": [
        ">In Assignment #3, you created five different feature families that capture annual and monthly aggregations. Here, you will model the retail problem with two approaches: using annual and monthly features. Therefore, you need to create the joined tables based on the following\n",
        "logic:\n",
        "\n",
        "|Table | annual features outputs | monthly features outputs|\n",
        "| ----------- | ----------- |----------- |\n",
        "|#1| annual_features.xlsx| mth_rolling_features.xlsx|\n",
        "|#2| annual_day_of_week_counts_pivot.xlsx|mth_day_counts.xlsx|\n",
        "|#3| |days_since_last_txn.xlsx|\n",
        "|#4| Retail_Data_Response.csv| Retail_Data_Response.csv|\n",
        "> In both the annual and monthly features approach, you need to join at the end with table #4, the clients responses. This is simply a table that contains the binary response of the\n",
        "client to our marketing effort as described above and that is the output or label or target\n",
        "that makes this a supervised learning problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW2BMe2wF0wl"
      },
      "source": [
        "### 1.2 Steps for each method (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BTrlrp7F2XF"
      },
      "source": [
        "> 1. Separate the inputs X and the output y in two data frames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE4GNPlEF50O"
      },
      "source": [
        "> 2. Split the data in train and test set. Use a test size value of 2/3 and set the random state\n",
        "equal to 1147 for consistency (i.e. the course code value). Use the following names for\n",
        "consistency.\n",
        "\n",
        "|Annual|Monthly|\n",
        "| ---------------- | ----------- |\n",
        "|X_train_annual  y_train_annual| X_train_monthly  y_train_monthly|\n",
        "|X_test_annual  y_test_annual|X_test_monthly  y_test_monthly|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgk7UYHfIAbe"
      },
      "source": [
        "> 3. Pre-process (if necessary for the method)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3GaPmIwIElb"
      },
      "source": [
        "> 4. Fit the training dataset and optimize the hyperparameters of the method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuVDA68sIFmv"
      },
      "source": [
        "> 5. Plot coeffcient values or feature importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNiojvUBIJZx"
      },
      "source": [
        "> 6. Plot probability distribution for test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0J6SHTgILdw"
      },
      "source": [
        "> 7. Plot confusion matrix and ROC curves of train/test set. Calculate precision/recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE-posxUIOYD"
      },
      "source": [
        "> 8. Plot decision boundary for top 2 features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8UrosIlIQdm"
      },
      "source": [
        "### 1.3 Comparison of methods (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mPaAYYyIU2M"
      },
      "source": [
        "> Compare the two feature engineering (annual and monthly) and the three modeling approaches (L1 log-reg, tree, forests) in terms of the outcomes of steps 5-8. Which combination of feature engineering and modeling approach do you select as the best to deploy in a\n",
        "production environment and why? Tabularize your findings in steps 5-8 to summarize the\n",
        "results and support your decision (how to organize information with tables in Markdown)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rp0Uj_nINgI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z-HpOXfC-bP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}